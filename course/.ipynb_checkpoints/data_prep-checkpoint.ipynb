{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac80e694-521e-4b50-a169-c4aae8ce8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037371b-db4b-4cd9-a782-3922cc245b2b",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd780dd-258b-4760-ba8a-4f3a195f4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    postfix = 'zip/refs/heads/main'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/{postfix}'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    \n",
    "    return repository_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff4c32f-69d3-4948-9d51-39f616ba9d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1217\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65ad21-709b-4d59-a257-7862601cd6a7",
   "metadata": {},
   "source": [
    "### Data Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf699f-2c74-46b4-b9d3-09fc335e4e08",
   "metadata": {},
   "source": [
    "#### Simple chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade77b78-d127-4343-a1c7-17731db739d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"\n",
    "    The following method creates a sliding window chunk based on a given sequence, based on size + step.\n",
    "        \n",
    "    :param str: Markdown text as a string\n",
    "    :param size: the size of a batch in that seq\n",
    "    :param step: a step, where if step = size, there's 0 overlap, and if step < size, there will be an overlap of \"step\"\n",
    "    \n",
    "    :return: List of chunks as strings\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5a0a4d-347d-4783-bdf0-e292509e8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b58abf-f2f5-482e-ab4d-6e82279289fb",
   "metadata": {},
   "source": [
    "#### Sections based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d19e715-b0ef-4b70-8845-cf59327f3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_markdown_by_level(text, level=2):\n",
    "#     \"\"\"\n",
    "#     Split markdown text by a specific header level.\n",
    "    \n",
    "#     :param text: Markdown text as a string\n",
    "#     :param level: Header level to split on\n",
    "#     :return: List of sections as strings\n",
    "#     \"\"\"\n",
    "#     # This regex matches markdown headers\n",
    "#     # For level 2, it matches lines starting with \"## \"\n",
    "#     header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "#     pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "#     # Split and keep the headers\n",
    "#     parts = pattern.split(text)\n",
    "    \n",
    "#     sections = []\n",
    "#     for i in range(1, len(parts), 3):\n",
    "#         # We step by 3 because regex.split() with\n",
    "#         # capturing groups returns:\n",
    "#         # [before_match, group1, group2, after_match, ...]\n",
    "#         # here group1 is \"## \", group2 is the header text\n",
    "#         header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "#         header = header.strip()\n",
    "\n",
    "#         # Get the content after this header\n",
    "#         content = \"\"\n",
    "#         if i+2 < len(parts):\n",
    "#             content = parts[i+2].strip()\n",
    "\n",
    "#         if content:\n",
    "#             section = f'{header}\\n\\n{content}'\n",
    "#         else:\n",
    "#             section = header\n",
    "#         sections.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25eeffca-15fe-43c9-abcb-37a775b9e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evidently_chunks = []\n",
    "\n",
    "# for doc in evidently_docs:\n",
    "#     doc_copy = doc.copy()\n",
    "#     doc_content = doc_copy.pop('content')\n",
    "#     print(doc_content)\n",
    "#     if doc_content:\n",
    "#         sections = split_markdown_by_level(doc_content, level=2)\n",
    "#         print(sections)\n",
    "#         for section in sections:\n",
    "#             section_doc = doc_copy.copy()\n",
    "#             section_doc['section'] = section\n",
    "#             evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176499ce-08de-4fdc-bd51-fc8ee5fa9e7e",
   "metadata": {},
   "source": [
    "#### LLM based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85927692-7331-478d-82ea-7783d39ecd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model=model):\n",
    "    \n",
    "    openai_client = OpenAI()\n",
    "    model = 'gpt-5-nano'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d677eb84-a84c-4118-801b-df15041fed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c68d687-0673-4858-8bd1-8f87a94c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "908cdcc7-51e3-4653-95aa-ea417de1bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3e057-3e4c-4780-b870-42a80e4681b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
