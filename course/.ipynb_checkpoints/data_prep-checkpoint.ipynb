{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac80e694-521e-4b50-a169-c4aae8ce8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "# Data chunking\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "# Search\n",
    "from minsearch import Index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from minsearch import VectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037371b-db4b-4cd9-a782-3922cc245b2b",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afd780dd-258b-4760-ba8a-4f3a195f4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com'\n",
    "    postfix = 'zip/refs/heads/main'\n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/{postfix}'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    \n",
    "    return repository_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ff4c32f-69d3-4948-9d51-39f616ba9d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1217\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65ad21-709b-4d59-a257-7862601cd6a7",
   "metadata": {},
   "source": [
    "### Data Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf699f-2c74-46b4-b9d3-09fc335e4e08",
   "metadata": {},
   "source": [
    "#### Simple chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ade77b78-d127-4343-a1c7-17731db739d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"\n",
    "    The following method creates a sliding window chunk based on a given sequence, based on size + step.\n",
    "        \n",
    "    :param str: Markdown text as a string\n",
    "    :param size: the size of a batch in that seq\n",
    "    :param step: a step, where if step = size, there's 0 overlap, and if step < size, there will be an overlap of \"step\"\n",
    "    \n",
    "    :return: List of chunks as strings\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae5a0a4d-347d-4783-bdf0-e292509e8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b58abf-f2f5-482e-ab4d-6e82279289fb",
   "metadata": {},
   "source": [
    "#### Sections based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d19e715-b0ef-4b70-8845-cf59327f3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_markdown_by_level(text, level=2):\n",
    "#     \"\"\"\n",
    "#     Split markdown text by a specific header level.\n",
    "    \n",
    "#     :param text: Markdown text as a string\n",
    "#     :param level: Header level to split on\n",
    "#     :return: List of sections as strings\n",
    "#     \"\"\"\n",
    "#     # This regex matches markdown headers\n",
    "#     # For level 2, it matches lines starting with \"## \"\n",
    "#     header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "#     pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "#     # Split and keep the headers\n",
    "#     parts = pattern.split(text)\n",
    "    \n",
    "#     sections = []\n",
    "#     for i in range(1, len(parts), 3):\n",
    "#         # We step by 3 because regex.split() with\n",
    "#         # capturing groups returns:\n",
    "#         # [before_match, group1, group2, after_match, ...]\n",
    "#         # here group1 is \"## \", group2 is the header text\n",
    "#         header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "#         header = header.strip()\n",
    "\n",
    "#         # Get the content after this header\n",
    "#         content = \"\"\n",
    "#         if i+2 < len(parts):\n",
    "#             content = parts[i+2].strip()\n",
    "\n",
    "#         if content:\n",
    "#             section = f'{header}\\n\\n{content}'\n",
    "#         else:\n",
    "#             section = header\n",
    "#         sections.append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25eeffca-15fe-43c9-abcb-37a775b9e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evidently_chunks = []\n",
    "\n",
    "# for doc in evidently_docs:\n",
    "#     doc_copy = doc.copy()\n",
    "#     doc_content = doc_copy.pop('content')\n",
    "#     print(doc_content)\n",
    "#     if doc_content:\n",
    "#         sections = split_markdown_by_level(doc_content, level=2)\n",
    "#         print(sections)\n",
    "#         for section in sections:\n",
    "#             section_doc = doc_copy.copy()\n",
    "#             section_doc['section'] = section\n",
    "#             evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176499ce-08de-4fdc-bd51-fc8ee5fa9e7e",
   "metadata": {},
   "source": [
    "#### LLM based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85927692-7331-478d-82ea-7783d39ecd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def llm(prompt, model=model):\n",
    "    \n",
    "#     openai_client = OpenAI()\n",
    "#     model = 'gpt-5-nano'\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "\n",
    "#     response = openai_client.responses.create(\n",
    "#         model=model,\n",
    "#         input=messages\n",
    "#     )\n",
    "\n",
    "#     return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d677eb84-a84c-4118-801b-df15041fed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"\n",
    "# Split the provided document into logical sections\n",
    "# that make sense for a Q&A system.\n",
    "\n",
    "# Each section should be self-contained and cover\n",
    "# a specific topic or concept.\n",
    "\n",
    "# <DOCUMENT>\n",
    "# {document}\n",
    "# </DOCUMENT>\n",
    "\n",
    "# Use this format:\n",
    "\n",
    "# ## Section Name\n",
    "\n",
    "# Section content with all relevant details\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## Another Section Name\n",
    "\n",
    "# Another section content\n",
    "\n",
    "# ---\n",
    "# \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c68d687-0673-4858-8bd1-8f87a94c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intelligent_chunking(text):\n",
    "#     prompt = prompt_template.format(document=text)\n",
    "#     response = llm(prompt)\n",
    "#     sections = response.split('---')\n",
    "#     sections = [s.strip() for s in sections if s.strip()]\n",
    "#     return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "908cdcc7-51e3-4653-95aa-ea417de1bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evidently_chunks = []\n",
    "\n",
    "# for doc in tqdm(evidently_docs):\n",
    "#     doc_copy = doc.copy()\n",
    "#     doc_content = doc_copy.pop('content')\n",
    "\n",
    "#     sections = intelligent_chunking(doc_content)\n",
    "#     for section in sections:\n",
    "#         section_doc = doc_copy.copy()\n",
    "#         section_doc['section'] = section\n",
    "#         evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89e29a-b0d1-4037-bb71-5b2146456b76",
   "metadata": {},
   "source": [
    "### Search Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2edfc-a1c1-477e-8dea-c5dd03dc2a7a",
   "metadata": {},
   "source": [
    "#### Text based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11d6d38a-fa99-41c2-ab5d-874969696612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x157837890>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98544bbb-61ee-4621-9dcb-a007fb83f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddb6f068-2db5-4402-917b-16245c317726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x15be78e10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7262f1b-adca-4782-b45a-4340bf48d8b5",
   "metadata": {},
   "source": [
    "#### vector based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af36370c-1854-45e8-b39c-f75f6b828a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee703eca-51a5-4a45-8a99-078a1592b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take from the 2nd Dataset (DE DTC FAQ) records which has a filename\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n",
    "\n",
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n",
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59d0d4b2-415a-4ec1-ad72-5e5b561ff77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be6a77c714c4c58bfd20d72b1764342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc29e364-5880-4a4e-a578-3448e03c265f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x157986190>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_vindex = VectorSearch(keyword_fields=[])\n",
    "\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee5a36ac-6ebf-4ac5-81e7-5aac8da9554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbddd4df-fb73-4b0f-a29b-9db8e1d7f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}\n"
     ]
    }
   ],
   "source": [
    "print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "765a1f31-1e31-4dd2-8f75-4e10d0d82493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615b8dc5710b455f9fb6591d067ee55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x1579868b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evidently\n",
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch(keyword_fields=[])\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7011c-3b7d-4779-aa82-346fae7ff5ff",
   "metadata": {},
   "source": [
    "#### Hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff907a45-a31f-44b0-97e2-55d1cfca7c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '33fc260cd8',\n",
       "  'question': 'Course: What can I do before the course starts?',\n",
       "  'sort_order': 5,\n",
       "  'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6728a5d5-88ff-4375-86fe-33608cc33c12",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m text_results = faq_index.search(query, num_results=\u001b[32m5\u001b[39m)\n\u001b[32m      5\u001b[39m q = embedding_model.encode(query)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m vector_results = \u001b[43mfaq_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m final_results = text_results + vector_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PyCharmProjects/aihero/course/.venv/lib/python3.13/site-packages/minsearch/minsearch.py:109\u001b[39m, in \u001b[36mIndex.search\u001b[39m\u001b[34m(self, query, filter_dict, boost_dict, num_results, output_ids)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.docs:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m query_vecs = {field: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_fields}\n\u001b[32m    110\u001b[39m scores = np.zeros(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.docs))\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Compute cosine similarity for each text field and apply boost\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PyCharmProjects/aihero/course/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:2129\u001b[39m, in \u001b[36mTfidfVectorizer.transform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   2112\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[32m   2113\u001b[39m \n\u001b[32m   2114\u001b[39m \u001b[33;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2125\u001b[39m \u001b[33;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[32m   2126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2127\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg=\u001b[33m\"\u001b[39m\u001b[33mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2129\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tfidf.transform(X, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PyCharmProjects/aihero/course/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1422\u001b[39m, in \u001b[36mCountVectorizer.transform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28mself\u001b[39m._check_vocabulary()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m _, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1424\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PyCharmProjects/aihero/course/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1264\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1263\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1266\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PyCharmProjects/aihero/course/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:104\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         doc = \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m         doc = tokenizer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PyCharmProjects/aihero/course/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:62\u001b[39m, in \u001b[36m_preprocess\u001b[39m\u001b[34m(doc, accent_function, lower)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mapply to a document.\u001b[39;00m\n\u001b[32m     45\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m \u001b[33;03m    preprocessed string\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     doc = \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     doc = accent_function(doc)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446b780-f761-4ebe-a115-aecfca8e4db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
